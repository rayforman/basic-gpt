{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiZdsOoNfjk45qAYXjuWR/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWodQ3N58EWd",
        "outputId": "57c84cc9-1b53-4f0a-fa6c-32a74ca0a025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-02 06:27:54--  https://www.gutenberg.org/files/2600/2600-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3359405 (3.2M) [text/plain]\n",
            "Saving to: ‘2600-0.txt.2’\n",
            "\n",
            "2600-0.txt.2        100%[===================>]   3.20M  14.6MB/s    in 0.2s    \n",
            "\n",
            "2024-03-02 06:27:55 (14.6 MB/s) - ‘2600-0.txt.2’ saved [3359405/3359405]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Start by downloading the dataset, here is War and Peace\n",
        "!wget https://www.gutenberg.org/files/2600/2600-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read .txt dataset in for inspection\n",
        "with open('2600-0.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "CKlOZ4l7ENDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length of the dataset\n",
        "print(\"Length of dataset (in characters): \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIezGuN0E3tr",
        "outputId": "55f0bf57-b391-48fd-e8c8-92cb58163ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset (in characters):  3227520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the beginning of the content\n",
        "# print(text[7454:10000])\n",
        "\n",
        "# Find the end of the content\n",
        "# print(text[-18413:])\n",
        "\n",
        "# Content begins at character 7454, disregard table of contents\n",
        "text = text[7454:-18413]"
      ],
      "metadata": {
        "id": "tQY7K-06FFei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 1000 characters to get a sense of style\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvpXLMPcFOVi",
        "outputId": "bdb16a1c-1792-4198-d4fc-7602c2604c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK ONE: 1805\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "“Well, Prince, so Genoa and Lucca are now just family estates of the\n",
            "Buonapartes. But I warn you, if you don’t tell me that this means war,\n",
            "if you still try to defend the infamies and horrors perpetrated by that\n",
            "Antichrist—I really believe he is Antichrist—I will have nothing\n",
            "more to do with you and you are no longer my friend, no longer my\n",
            "‘faithful slave,’ as you call yourself! But how do you do? I see I\n",
            "have frightened you—sit down and tell me all the news.”\n",
            "\n",
            "It was in July, 1805, and the speaker was the well-known Anna Pávlovna\n",
            "Schérer, maid of honor and favorite of the Empress Márya Fëdorovna.\n",
            "With these words she greeted Prince Vasíli Kurágin, a man of high\n",
            "rank and importance, who was the first to arrive at her reception. Anna\n",
            "Pávlovna had had a cough for some days. She was, as she said, suffering\n",
            "from la grippe; grippe being then a new word in St. Petersburg, used\n",
            "only by the elite.\n",
            "\n",
            "All her invitations without exception, written in French, and de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all unique characters that occur in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Print the list of the chars and vocab size\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QMrqg4FGUMb",
        "outputId": "51ef2733-9dd2-48ac-a605-ea99249e50f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÀÁÉàáâäæçèéêëíîïóôöúüýœ—‘’“”\n",
            "104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we tokenize the text, map characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}  # character -> integer map\n",
        "itos = {i:ch for i,ch in enumerate(chars)}  # integer -> character map\n",
        "\n",
        "# Encoders and decoders for string -> list[int], and vice versa\n",
        "encode = lambda s: [stoi[c] for c in s]           # encoder, takes a string and outputs a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # decoder, takes a list of integers and outputs a string\n",
        "\n",
        "# Test functionality\n",
        "print(encode(\"good morning\"))\n",
        "print(decode(encode(\"good morning\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_r-1c1FGndD",
        "outputId": "e5c24513-a81b-4b37-c5dc-35314cdac061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56, 64, 64, 53, 1, 62, 64, 67, 63, 58, 63, 56]\n",
            "good morning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now encode the entire dataset into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Print tensor shape and data type\n",
        "print(\"Shape of data tensor: \", data.shape)\n",
        "print(\"Data tensor data type: \", data.dtype)\n",
        "\n",
        "# Print first 1000 characters encoded as integers, with max being vocab_size-1\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uDXsJIGINtV",
        "outputId": "b11e19a7-82c8-4a3e-c61f-73fa074b9577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor:  torch.Size([3201653])\n",
            "Data tensor data type:  torch.int64\n",
            "tensor([ 25,  38,  38,  34,   1,  38,  37,  28,  20,   1,  11,  18,  10,  15,\n",
            "          0,   0,   0,   0,   0,   0,  26,  31,  24,  39,  43,  28,  41,   1,\n",
            "         32,   0,   0, 102,  46,  54,  61,  61,   6,   1,  39,  67,  58,  63,\n",
            "         52,  54,   6,   1,  68,  64,   1,  30,  54,  63,  64,  50,   1,  50,\n",
            "         63,  53,   1,  35,  70,  52,  52,  50,   1,  50,  67,  54,   1,  63,\n",
            "         64,  72,   1,  59,  70,  68,  69,   1,  55,  50,  62,  58,  61,  74,\n",
            "          1,  54,  68,  69,  50,  69,  54,  68,   1,  64,  55,   1,  69,  57,\n",
            "         54,   0,  25,  70,  64,  63,  50,  65,  50,  67,  69,  54,  68,   8,\n",
            "          1,  25,  70,  69,   1,  32,   1,  72,  50,  67,  63,   1,  74,  64,\n",
            "         70,   6,   1,  58,  55,   1,  74,  64,  70,   1,  53,  64,  63, 101,\n",
            "         69,   1,  69,  54,  61,  61,   1,  62,  54,   1,  69,  57,  50,  69,\n",
            "          1,  69,  57,  58,  68,   1,  62,  54,  50,  63,  68,   1,  72,  50,\n",
            "         67,   6,   0,  58,  55,   1,  74,  64,  70,   1,  68,  69,  58,  61,\n",
            "         61,   1,  69,  67,  74,   1,  69,  64,   1,  53,  54,  55,  54,  63,\n",
            "         53,   1,  69,  57,  54,   1,  58,  63,  55,  50,  62,  58,  54,  68,\n",
            "          1,  50,  63,  53,   1,  57,  64,  67,  67,  64,  67,  68,   1,  65,\n",
            "         54,  67,  65,  54,  69,  67,  50,  69,  54,  53,   1,  51,  74,   1,\n",
            "         69,  57,  50,  69,   0,  24,  63,  69,  58,  52,  57,  67,  58,  68,\n",
            "         69,  99,  32,   1,  67,  54,  50,  61,  61,  74,   1,  51,  54,  61,\n",
            "         58,  54,  71,  54,   1,  57,  54,   1,  58,  68,   1,  24,  63,  69,\n",
            "         58,  52,  57,  67,  58,  68,  69,  99,  32,   1,  72,  58,  61,  61,\n",
            "          1,  57,  50,  71,  54,   1,  63,  64,  69,  57,  58,  63,  56,   0,\n",
            "         62,  64,  67,  54,   1,  69,  64,   1,  53,  64,   1,  72,  58,  69,\n",
            "         57,   1,  74,  64,  70,   1,  50,  63,  53,   1,  74,  64,  70,   1,\n",
            "         50,  67,  54,   1,  63,  64,   1,  61,  64,  63,  56,  54,  67,   1,\n",
            "         62,  74,   1,  55,  67,  58,  54,  63,  53,   6,   1,  63,  64,   1,\n",
            "         61,  64,  63,  56,  54,  67,   1,  62,  74,   0, 100,  55,  50,  58,\n",
            "         69,  57,  55,  70,  61,   1,  68,  61,  50,  71,  54,   6, 101,   1,\n",
            "         50,  68,   1,  74,  64,  70,   1,  52,  50,  61,  61,   1,  74,  64,\n",
            "         70,  67,  68,  54,  61,  55,   2,   1,  25,  70,  69,   1,  57,  64,\n",
            "         72,   1,  53,  64,   1,  74,  64,  70,   1,  53,  64,  23,   1,  32,\n",
            "          1,  68,  54,  54,   1,  32,   0,  57,  50,  71,  54,   1,  55,  67,\n",
            "         58,  56,  57,  69,  54,  63,  54,  53,   1,  74,  64,  70,  99,  68,\n",
            "         58,  69,   1,  53,  64,  72,  63,   1,  50,  63,  53,   1,  69,  54,\n",
            "         61,  61,   1,  62,  54,   1,  50,  61,  61,   1,  69,  57,  54,   1,\n",
            "         63,  54,  72,  68,   8, 103,   0,   0,  32,  69,   1,  72,  50,  68,\n",
            "          1,  58,  63,   1,  33,  70,  61,  74,   6,   1,  11,  18,  10,  15,\n",
            "          6,   1,  50,  63,  53,   1,  69,  57,  54,   1,  68,  65,  54,  50,\n",
            "         60,  54,  67,   1,  72,  50,  68,   1,  69,  57,  54,   1,  72,  54,\n",
            "         61,  61,   7,  60,  63,  64,  72,  63,   1,  24,  63,  63,  50,   1,\n",
            "         39,  80,  71,  61,  64,  71,  63,  50,   0,  42,  52,  57,  86,  67,\n",
            "         54,  67,   6,   1,  62,  50,  58,  53,   1,  64,  55,   1,  57,  64,\n",
            "         63,  64,  67,   1,  50,  63,  53,   1,  55,  50,  71,  64,  67,  58,\n",
            "         69,  54,   1,  64,  55,   1,  69,  57,  54,   1,  28,  62,  65,  67,\n",
            "         54,  68,  68,   1,  36,  80,  67,  74,  50,   1,  29,  88,  53,  64,\n",
            "         67,  64,  71,  63,  50,   8,   0,  46,  58,  69,  57,   1,  69,  57,\n",
            "         54,  68,  54,   1,  72,  64,  67,  53,  68,   1,  68,  57,  54,   1,\n",
            "         56,  67,  54,  54,  69,  54,  53,   1,  39,  67,  58,  63,  52,  54,\n",
            "          1,  45,  50,  68,  89,  61,  58,   1,  34,  70,  67,  80,  56,  58,\n",
            "         63,   6,   1,  50,   1,  62,  50,  63,   1,  64,  55,   1,  57,  58,\n",
            "         56,  57,   0,  67,  50,  63,  60,   1,  50,  63,  53,   1,  58,  62,\n",
            "         65,  64,  67,  69,  50,  63,  52,  54,   6,   1,  72,  57,  64,   1,\n",
            "         72,  50,  68,   1,  69,  57,  54,   1,  55,  58,  67,  68,  69,   1,\n",
            "         69,  64,   1,  50,  67,  67,  58,  71,  54,   1,  50,  69,   1,  57,\n",
            "         54,  67,   1,  67,  54,  52,  54,  65,  69,  58,  64,  63,   8,   1,\n",
            "         24,  63,  63,  50,   0,  39,  80,  71,  61,  64,  71,  63,  50,   1,\n",
            "         57,  50,  53,   1,  57,  50,  53,   1,  50,   1,  52,  64,  70,  56,\n",
            "         57,   1,  55,  64,  67,   1,  68,  64,  62,  54,   1,  53,  50,  74,\n",
            "         68,   8,   1,  42,  57,  54,   1,  72,  50,  68,   6,   1,  50,  68,\n",
            "          1,  68,  57,  54,   1,  68,  50,  58,  53,   6,   1,  68,  70,  55,\n",
            "         55,  54,  67,  58,  63,  56,   0,  55,  67,  64,  62,   1,  61,  50,\n",
            "          1,  56,  67,  58,  65,  65,  54,  21,   1,  56,  67,  58,  65,  65,\n",
            "         54,   1,  51,  54,  58,  63,  56,   1,  69,  57,  54,  63,   1,  50,\n",
            "          1,  63,  54,  72,   1,  72,  64,  67,  53,   1,  58,  63,   1,  42,\n",
            "         69,   8,   1,  39,  54,  69,  54,  67,  68,  51,  70,  67,  56,   6,\n",
            "          1,  70,  68,  54,  53,   0,  64,  63,  61,  74,   1,  51,  74,   1,\n",
            "         69,  57,  54,   1,  54,  61,  58,  69,  54,   8,   0,   0,  24,  61,\n",
            "         61,   1,  57,  54,  67,   1,  58,  63,  71,  58,  69,  50,  69,  58,\n",
            "         64,  63,  68,   1,  72,  58,  69,  57,  64,  70,  69,   1,  54,  73,\n",
            "         52,  54,  65,  69,  58,  64,  63,   6,   1,  72,  67,  58,  69,  69,\n",
            "         54,  63,   1,  58,  63,   1,  29,  67,  54,  63,  52,  57,   6,   1,\n",
            "         50,  63,  53,   1,  53,  54])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to split the data for training and validation\n",
        "percent_training = 0.9                  # using 90% for training, this can change\n",
        "n = int(percent_training * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "hO_QW-NEIdRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine block size, the maximum length of each chunk of the dataset to be worked on at a given time\n",
        "block_size = 8    # choosing 8, this can change\n",
        "\n",
        "# Print the first block_size+1 in the training data\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J008ZRQCKAgz",
        "outputId": "21a5a9fc-2073-421e-8707-d062822e1c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([25, 38, 38, 34,  1, 38, 37, 28, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the desired pattern of prediction for each chunk of data\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]    # target is the token following the context\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]         # y is staggered one past x\n",
        "  print(f\"when the input/context is {context.tolist()}, the output/target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPFA3AZIK-BX",
        "outputId": "c0aab6c5-26a9-443b-bb63-b2f05057ba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when the input/context is [25], the output/target is: 38\n",
            "when the input/context is [25, 38], the output/target is: 38\n",
            "when the input/context is [25, 38, 38], the output/target is: 34\n",
            "when the input/context is [25, 38, 38, 34], the output/target is: 1\n",
            "when the input/context is [25, 38, 38, 34, 1], the output/target is: 38\n",
            "when the input/context is [25, 38, 38, 34, 1, 38], the output/target is: 37\n",
            "when the input/context is [25, 38, 38, 34, 1, 38, 37], the output/target is: 28\n",
            "when the input/context is [25, 38, 38, 34, 1, 38, 37, 28], the output/target is: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extend this to have multiple batches by adding a batch dimension.\n",
        "# Stack multiple sequences into single tensor for efficiency-- processed independently.\n",
        "\n",
        "# Randomly generate sequences from the dataset\n",
        "torch.manual_seed(1234)\n",
        "batch_size = 4            # Number of independent sequences that are processed in parallel\n",
        "block_size = 8            # Maximum context length for predictions\n",
        "\n",
        "# Generate small batch of data of inputs x and targets y\n",
        "def get_batch(split):\n",
        "\n",
        "  # are we sampling from train or val split\n",
        "  data = train_data if split == 'train' else val_data\n",
        "\n",
        "  # Creates a randomized 1-dimensional tensor (vector) with batch_size elements,\n",
        "  # each being a random offset that indicates the position of each block\n",
        "  ix = torch.randint(high=len(data)-block_size, size=(batch_size,))\n",
        "\n",
        "  # Stack input sequences into a tensor x, and targets in a tensor y,\n",
        "  # stacking them as rows in their respective tensor.\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "  return x, y\n"
      ],
      "metadata": {
        "id": "7VIgX3-xMpkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets get a batch\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Print the input shape and tensor\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "# Print the output shape and tensor\n",
        "print('outputs:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# 32 independent examples into one batch (4 random sequences of 8 characters)\n",
        "for b in range(batch_size):     # B: batch dimension\n",
        "  for t in range(block_size):   # T: \"time\" dimension (time steps)\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when the input/context is {context.tolist()}, the output/target is: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ryl_Y6vSUqRp",
        "outputId": "a8189569-ed43-4f0e-f609-99b0a959987b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[54, 58, 56, 57, 51, 64, 67, 68],\n",
            "        [69,  1, 65, 64, 67, 52, 57,  8],\n",
            "        [62, 50, 61, 61,  1, 52, 61, 54],\n",
            "        [ 1, 53, 58, 55, 55, 54, 67, 54]])\n",
            "outputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[58, 56, 57, 51, 64, 67, 68,  1],\n",
            "        [ 1, 65, 64, 67, 52, 57,  8,  1],\n",
            "        [50, 61, 61,  1, 52, 61, 54, 50],\n",
            "        [53, 58, 55, 55, 54, 67, 54, 63]])\n",
            "\n",
            "\n",
            "when the input/context is [54], the output/target is: 58\n",
            "when the input/context is [54, 58], the output/target is: 56\n",
            "when the input/context is [54, 58, 56], the output/target is: 57\n",
            "when the input/context is [54, 58, 56, 57], the output/target is: 51\n",
            "when the input/context is [54, 58, 56, 57, 51], the output/target is: 64\n",
            "when the input/context is [54, 58, 56, 57, 51, 64], the output/target is: 67\n",
            "when the input/context is [54, 58, 56, 57, 51, 64, 67], the output/target is: 68\n",
            "when the input/context is [54, 58, 56, 57, 51, 64, 67, 68], the output/target is: 1\n",
            "when the input/context is [69], the output/target is: 1\n",
            "when the input/context is [69, 1], the output/target is: 65\n",
            "when the input/context is [69, 1, 65], the output/target is: 64\n",
            "when the input/context is [69, 1, 65, 64], the output/target is: 67\n",
            "when the input/context is [69, 1, 65, 64, 67], the output/target is: 52\n",
            "when the input/context is [69, 1, 65, 64, 67, 52], the output/target is: 57\n",
            "when the input/context is [69, 1, 65, 64, 67, 52, 57], the output/target is: 8\n",
            "when the input/context is [69, 1, 65, 64, 67, 52, 57, 8], the output/target is: 1\n",
            "when the input/context is [62], the output/target is: 50\n",
            "when the input/context is [62, 50], the output/target is: 61\n",
            "when the input/context is [62, 50, 61], the output/target is: 61\n",
            "when the input/context is [62, 50, 61, 61], the output/target is: 1\n",
            "when the input/context is [62, 50, 61, 61, 1], the output/target is: 52\n",
            "when the input/context is [62, 50, 61, 61, 1, 52], the output/target is: 61\n",
            "when the input/context is [62, 50, 61, 61, 1, 52, 61], the output/target is: 54\n",
            "when the input/context is [62, 50, 61, 61, 1, 52, 61, 54], the output/target is: 50\n",
            "when the input/context is [1], the output/target is: 53\n",
            "when the input/context is [1, 53], the output/target is: 58\n",
            "when the input/context is [1, 53, 58], the output/target is: 55\n",
            "when the input/context is [1, 53, 58, 55], the output/target is: 55\n",
            "when the input/context is [1, 53, 58, 55, 55], the output/target is: 54\n",
            "when the input/context is [1, 53, 58, 55, 55, 54], the output/target is: 67\n",
            "when the input/context is [1, 53, 58, 55, 55, 54, 67], the output/target is: 54\n",
            "when the input/context is [1, 53, 58, 55, 55, 54, 67, 54], the output/target is: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input to the transformer\n",
        "print(xb)"
      ],
      "metadata": {
        "id": "I1Q1ERneVU-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ae6989-302f-4c0b-bbd2-756d6b0b3f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[54, 58, 56, 57, 51, 64, 67, 68],\n",
            "        [69,  1, 65, 64, 67, 52, 57,  8],\n",
            "        [62, 50, 61, 61,  1, 52, 61, 54],\n",
            "        [ 1, 53, 58, 55, 55, 54, 67, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the simplest language model, a Bigram Language Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    # each token reads the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    # idx and targets are both (B,T) tensor of integers (Batch #, Time #)\n",
        "    # pass inputs through embedding layer to get logits\n",
        "    logits = self.token_embedding_table(idx)    # (B,T,C)\n",
        "\n",
        "    #\n",
        "    if targets is None:\n",
        "      # allows for optional targets, otherwise just unlabeled generation\n",
        "      loss = None\n",
        "    else:\n",
        "      # Reshaping since PyTorch expects channel in 2nd dimension\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  # Where max_new_tokens is the number of tokens to be generated.\n",
        "  # This function takes a (B,T) sequence and extends it to be (B,T+1),\n",
        "  # then to (B,T+2), etc. until reaching (B,T+max_new_tokens).\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      # get the predictions with current indices, ignore loss.\n",
        "      logits, loss = self(idx)\n",
        "      # focus on only last time step, pluck out last element in T\n",
        "      logits = logits[:, -1, :]\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      # get single prediction from softmax distribution for next token in sequence\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      # append sampled index to running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "Zex4j3lLlmsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the BLM Model\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "# Print resulting shape and loss\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# Generate 100 characters from a single newline character, 1 batch and 1 time.\n",
        "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCxd9G6BvPQA",
        "outputId": "282f2935-5818-4266-f45e-bae33403ed8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 104])\n",
            "tensor(5.3348, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "íA=öWêkUäKáp:Tv\n",
            "Y7?;âQægïöx*’=àczNöEshLiCä*08ZzHç“zJ êC AT;Tóó2’î-ëUýü2*OrA-”ólëThKZh‘RM?sæcTtf3;”P7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is meaningless output before training a model. Represents pure chance and is currently only using a single character to predict the next."
      ],
      "metadata": {
        "id": "CcpEGQYn01Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)    # small network, can afford to go higher than 3e-4"
      ],
      "metadata": {
        "id": "5aF8VwqW2acC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Typical Training Loop\n",
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "  # Get a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # Evaluate loss\n",
        "  logits, loss = m(xb, yb)                  # get prediction and loss\n",
        "  optimizer.zero_grad(set_to_none=True)     # zero model parameters to prevent accumulation\n",
        "  loss.backward()                           # back propagation\n",
        "  optimizer.step()                          # step in direction of negative gradient\n",
        "\n",
        "\n",
        "# Print resulting loss from training\n",
        "print(loss.item())\n",
        "\n",
        "# Increasing number of iterations will decrease loss, until plateau due to very simple model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpzL29O623es",
        "outputId": "72f18b02-d2d9-421a-9a9d-5d2363c302bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.383948802947998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print resulting generation after training (500 characters)\n",
        "print(decode(m.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=400)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br9oKXVP4arJ",
        "outputId": "591647ec-1ea3-424d-f622-033a4bafdde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Namesthe TEhethe He to fof stesimexe lirs toft o ent\n",
            "ou witat and “HAne r. ifr, bimomras. ale\n",
            "Dón enst thed depleanererdstthercoosh t che tusioo ay owit nca and y Pimuinthanes a buthe, bumbullily aply, sed sompsove, owe he Sóng het g ous g whanopo taly ooly s hesefe ctooppeincessache s bst\n",
            "rss, es, Palimol e\n",
            "dour\n",
            "olindid atighisuny ing ar s h! Am, s tht k, sst\n",
            "Med poised s avan heve boflúgot fokie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce ATTENTION: TRANSFORMERS\n"
      ],
      "metadata": {
        "id": "G2U2F7TO5Adl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}